---
title: che168全站爬取
date: 2023-02-20 13:13:17
tags:
---
# 学习scrapy二手车网站爬取
## 分析网站源代码
点击键盘f2查看源代码，可以找到详情页的超链接在ul标签下的每一个li标签内a标签的href属性，通过xpath在程序中定位每辆车详情页超链接，通过yield 关键词返回scrapy.Request()方法，发送获取的详情页请求。
![如图](img/屏幕截图 2023-02-20 150345.png#pic_center =300x100 ''图片title'')
## 创建scrapy项目
进入prompt，输入命令scrapy startproject ershouche,下一步进入到ershouche项目中，创建爬虫py文件，输入命令scrapy genspider car 168.com，完成项目的创建。

## 更改配置
首先更改settings中的几个设置值；
 `ROBOTSTXT_OBEY = False`
 `LOG_LEVEL = 'WARNING'`
 当ROBOTSTXT_OBEY的值为True是，，表示遵循robot协议，否则不在遵循。
 LOG_LEVEL的值有以下几种：
 种类|功能
 ---|:--
 DEBUG|指出细粒度信息事件对调试应用程序，详细查看运行情况的则开启debug。
 INFO|在粗粒度级别上突出强调应用程序的运行过程
  WARN|表明会出现潜在错误的情形
  ERROR|发生错误事件，但仍然不影响系统的继续运行
  FATAL|指出某个严重的错误事件将会导致应用程序的退出。

## 主要代码
### 使用ErshoucheItem提取器解析网页

\```
import scrapy
from ershouche.items import ErshoucheItem
from scrapy.linkextractors import LinkExtractor

class CarSpider(scrapy.Spider):
    name = 'car'
    allowed_domains = ['che168.com']
    start_urls = ['https://www.che168.com/zhangjiakou/list/#pvareaid=100945']

    def parse(self, response):
       le = LinkExtractor(restrict_xpaths=("//ul[@class='viewlist_ul']/li"))
       links = le.extract_links(response)
       for i,link in enumerate(links):
           print(i,"\t",link.text.replace(" ","").strip(),link.url)
           yield scrapy.Request(
               url = link.url,
               callback = self.detail_parse)
       page_le = LinkExtractor(restrict_xpaths="//div[@class='page fn-clear']/a")
       page_links = page_le.extract_links(response)
       for i,page in enumerate(page_links): 
           yield scrapy.Request(url=page.url,callback=self.parse) 
    def detail_parse(self,response):
        print(response)
\```

###使用crawlspider框架解析网页

\```
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
class ErshouSpider(CrawlSpider):
    name = 'ershou'
    allowed_domains = ['che168.com']
    start_urls = ['https://www.che168.com/zhangjiakou/list/#pvareaid=100945']

    rules = (
        Rule(LinkExtractor(restrict_xpaths="//ul[@class='viewlist_ul']/li"), callback='parse_item', follow=False),#follow表示是否继续在当前页面找
        Rule(LinkExtractor(restrict_xpaths="//div[@class='page fn-clear']/a"), follow=True)
    )

    def parse_item(self, response):
        title = response.xpath('//div[@class="car-box"]/h3/text()').extract_first()
        price = response.xpath('//span[@class = "price"]/text()').extract_first()
        print(title,"\t",price)
\```

## 总结
### ErshoucheItem提取器详细介绍
创建LinkExtractor对象
`le = LinkExtractor()`
参数介绍：

形参|作用|
----|:---|
allow|接受一个正则表达式或者是正则表达式列表，提取绝对url与正则匹配的链接。|
deny|接受一个正则表达式或一个正则表达式列表，不获取与正则匹配的链接|
allow_domains|接受一个域名或是域名列表，提取域名中的链接|
deny_domains|接受一个域名或域名列表，不提取属于这些域名内的链接|
restrict_xpaths|接收xpath表达式，提取表达式选中区域的链接|
restrict_css参数|接收css表达式，提取表达式中选中区域的链接|
tags|接收一个标签或标签列表，提取标签内的列表默认为['a','area']|
attrs|接收一个属性或属性列表，指定属性内的链接默认为['href']|
process_value|用来回调函数，处理javascript代码|
使用LinkExtractor对象的extract_links方法依据创建对象所描述的规则提取对象中包含的链接，并返列表每个元素都是Link对象，Link.url可以获取链接，Linktext获取文本内容。|


### crawlspider框架总结

crawlspider用于全站爬取那些具有一定规则网站的常用爬虫，继承spider，有一些独特属性，其中rules是Rule的集合，用于匹配目标网站并排除干扰，其参数作用有一下几点：
形参|作用
---|:---
allow|满足括号中的正则表达式会被提取，如果为空全部提取。
deny|满足括号中正则表达式的不会被提取
allow——domains|在参数值中的域名会被提取，否则不被允许发送请求
deny_domains|一定不会被请求到的域名
restrict_xpaths|使用xpath表达式获取选择区域中的链接
-----
暂时学到的东西，之后会继续补充；








