---
title: 分布式爬虫
date: 2023-02-26 15:06:50
tags:
---
## 安装分布式爬虫模块

`pip install scrapy-redis`

## 创建爬虫项目

1. 创建项目命令：
` scrapy startproject tianya `
2. cd命令进入文件tianya后，创建spider爬虫文件
`cd tianya`
`scrapy genspider ty tianya.cn`


## settings配置

将是否顺从robot协议的值该为false，并且将log等级该为提醒，取消注释item_pipeline.


` ROBOTSTXT_OBEY = False `
` LOG_LEVEL = 'WARNING '`

分布式爬虫固定配置信息：

1. 使用向scrapy_redis模块提供的RedisPipeline。如下代码
\```

	ITEM_PIPELINES = {
	    'scrapy_redis.pipelines.RedisPipeline':301,
	    'tianya2.pipelines.Tianya2Pipeline': 300,
	}
\```

2. redis连接需要的相关信息，REDIS_DB表示使用的数据库序号。

\```

	REDIS_PORT = 6379
	REDIS_HOST = 'localhost'
	REDIS_DB = 9
\```

3. SCHEDULER的值表示使用scrapy_redis模块提供的调度器，SCHEDULER_PERSIST指如果为真在关闭时自动保存请求信息，否则清空请求信息，DUPEFILTER_CLASS表示scrapy_redis提供的过滤器

\```

SCHEDULER = 'scrapy_redis.scheduler.Scheduler'
SCHEDULER_PERSIST = True
DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'
\```

## 目标网页url

普通的scrapy爬虫起始爬取页面放到start_url中，分布式爬虫的特点有多台计算机同时进行爬取，这时的初始爬取页面就不能放在start——url中，会造成多台计算机爬取到大量相同的url。（这里需要了解分布式爬虫的原理部分，在前面提到）这里会用到redis_key来代替start_url。redis_key指向调度器中最初使的url。
初始url添加方式：
1) 直接打开redis可视化端，添加redis_key的键，并将初始页面作为值放入当前键中。
2) 打开cmd，连接到数据库之后，输入命令:select REDIS_DB,lpush ty_start_url 初始url

## 解析页面的主要代码（）
分布式爬虫的解析页面与前面学到的相同，这里不在说明，需要注意的是：之前Ty2Spider继承的是scrapy.spiders,分布式爬虫需要导入scrapy_redis.RedisSpider。并且不需要判断url是否重复。代码如下：

\```

	class Ty2Spider(RedisSpider):
    name = 'ty2'
    allowed_domains = ['tianya.cn']
    #start_urls = ['http://bbs.tianya.cn/list-worldlook-1.shtml']
    redis_key = "ty_start_url"
    
    def __init__(self,name=None,**kwargs):
        self.redis = Redis(decode_responses = True)
        super(Ty2Spider,self).__init__(name)

    def parse(self, response):
        print(response)
        #print(response.text)
        trs = response.xpath("//div[@class='mt5']//tr[position()>1]/td[1]")
        for i,tr in enumerate(trs):
            href = tr.xpath("./a/@href").extract_first()
            yield scrapy.Request(
                url = response.urljoin(href),
                method = "get",
                callback = self.parse_detail
                )
        
        #分页操作
        next_page = response.xpath('//div[@class="short-pages-2 clearfix"]/div[1]/a[2]/@href').extract_first()
        if next_page:
            yield scrapy.Request(
                url = response.urljoin(next_page),
                callback = self.parse)
            
    def parse_detail(self,response):
        title = response.xpath('//span[@class="s_title"]/span/text()').extract_first()
        content = response.xpath('//div[@class="atl-content"]/div[2]/div/text()').extract_first()
        print(title)
        item = TianyaItem()
        item["title"] = title
        item['content'] = content
        yield item

\```

